{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the onboarding tutorial\n",
    "\n",
    "We will run some short examples to get you familiar with the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Copy the hdfs config files to the session\n",
    "To connect to hdfs for the first time, we need to copy the hdfs-site.xml and core-site.xml files until this process is automated.\n",
    "\n",
    "In the file overview, open a terminal by clicking the icon in the top right corner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-- 1 cdsw cdsw 570 Apr 14 07:08 getHDFSSite.sh\r\n"
     ]
    }
   ],
   "source": [
    "#Making the file executable\n",
    "!ls -la getHDFSSite.sh\n",
    "!chmod ug+x getHDFSSite.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal please execute the provided script by typing ./getHDFSSite.sh (or using tab to autocomplete).\n",
    "When prompted, please enter your workspace password, to complete the file transfer.\n",
    "\n",
    "If this does not work, because the connection is taking some time, please try executing this script in a non-jupyter session.\n",
    "\n",
    "The config files are stored in the conf folder, so in any future sessions it is enough to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/conf/*site.xml /etc/hadoop/conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Kernels\n",
    "Currently the jupyter notebook is only configured for python.\n",
    "\n",
    "To use R/Scala please switch to the Workbench editor, if you wish to use the relevant magic words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python dependency management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting argparse\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "cat: requirements.txt: No such file or directory\n",
      "/bin/sh: ./cdsw-build.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using pip to install python packages\n",
    "!pip3 install argparse\n",
    "\n",
    "# Manage your depencencies in requirements.txt\n",
    "!cat requirements.txt\n",
    "\n",
    "# Run startup script in interactive sessions\n",
    "!./cdsw-build.sh\n",
    "\n",
    "# List installed packages\n",
    "# !pip3 list installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some python examples\n",
    "\n",
    "### Estimating Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.145000\n"
     ]
    }
   ],
   "source": [
    "#Estimating Pi\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "\n",
    "partitions = 2\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "| Third|  3|\n",
      "|Fourth|  4|\n",
      "| Fifth|  5|\n",
      "| First|  1|\n",
      "|Second|  2|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to HDFS\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import sys, re, subprocess\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"HDFSTest\")\\\n",
    "    .getOrCreate()\n",
    " \n",
    "spark.sparkContext._conf.getAll()\n",
    "\n",
    "REMOTE_HDFS_MASTER = 'tst-env-public-lake-master1.tst-env.hm0v-a3xe.cloudera.site'\n",
    "\n",
    "data = [('First', 1), ('Second', 2), ('Third', 3), ('Fourth', 4), ('Fifth', 5)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.write.csv(path=\"hdfs://\" + REMOTE_HDFS_MASTER + \"/tmp/csv_example\", mode=\"overwrite\")\n",
    "\n",
    "df_load = spark.read.csv(\"hdfs://\" + REMOTE_HDFS_MASTER + \"/tmp/csv_example\")\n",
    "\n",
    "df_load.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to an S3 bucket\n",
    "\n",
    "To connect to S3 with your default credentials, the path must exist.\n",
    "\n",
    "The default path is s3a://caap-cloudera-data/cdp_datalake/projects/<project-group>/<project-name>/\n",
    "    \n",
    "Depending on your permissions, you may not be able to access the project folder 'raw'.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "| Third|  3|\n",
      "|Fourth|  4|\n",
      "| Fifth|  5|\n",
      "| First|  1|\n",
      "|Second|  2|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to S3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.executor.instances\", 2)\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://caap-cloudera-data/cdp_datalake/cdp-test/\")\\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext._conf.getAll()\n",
    "\n",
    "data = [('First', 1), ('Second', 2), ('Third', 3), ('Fourth', 4), ('Fifth', 5)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "df.write.format(\"csv\").mode(\"Overwrite\").save(\"s3a://caap-cloudera-data/cdp_datalake/cdp-test/test.csv\")\n",
    "\n",
    "df_load = spark.read.csv(\"s3a://caap-cloudera-data/cdp_datalake/cdp-test/test.csv\")\n",
    "df_load.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Hive\n",
    "\n",
    "via spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|           default|\n",
      "|information_schema|\n",
      "|               sys|\n",
      "|              test|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row,StructField,StructType,StringType,IntegerType\n",
    "import sys, re, subprocess\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PythonSQL\").config(\"spark.executor.instances\", 2).config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://caap-cloudera-data/cdp_datalake/warehouse/tablespace/external/hive/\").config(\"spark.sql.warehouse.dir\", \"s3://caap-cloudera-data/cdp_datalake/warehouse/tablespace/external/hive/\").config(\"hive.mapred.supports.subdirectories\", \"true\").config(\"hive.supports.subdirectories\", \"true\").config(\"mapred.input.dir.recursive\", \"true\").getOrCreate()\n",
    "spark.sql(\"SHOW databases\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Spark configs.\n",
    "\n",
    "As you saw in some of the examples, you can provide custom configs for spark.\n",
    "\n",
    "On a project basis, the global settings are defined in the file spark-defaults.conf\n",
    "\n",
    "You can also take a look at the docs: https://docs.cloudera.com/machine-learning/cloud/spark/topics/ml-managing-dependencies-for-spark-2-jobs.html\n",
    "\n",
    "They provide some examples about adding files and other dependencies to spark as well as modifying loglevels and other settings.\n",
    "\n",
    "Please note that some settings are set at session start up and may only take effect in a new session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you and have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
